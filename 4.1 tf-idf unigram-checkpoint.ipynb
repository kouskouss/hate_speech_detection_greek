{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cfe0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PRE-PROCESSING\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# TD-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TRAIN/TEST SET\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5ebecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"el_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a64b20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Η απορία της ημέρας!!\\n\\nΓιατί σε φωτογραφία π...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ρε κόψτε την σύνδεση με τον βλακα Καραγιάννη έ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ότι ήσουν μαλάκω το ξέραμε. Ότι ήσουν αμπαλη ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ΠΟΙΟΣ ΓΑΜΙΟΛΗΣ ΚΡΥΒΕΤΑΙ ΠΙΣΩ ΑΠΤΗΝΕ ΑΝΑΠΑΤΕΧΗ ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Βλεπω αυτόν τον πατέρα στην Σκατιανα και μου έ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  class\n",
       "0  Η απορία της ημέρας!!\\n\\nΓιατί σε φωτογραφία π...    0.0\n",
       "1  Ρε κόψτε την σύνδεση με τον βλακα Καραγιάννη έ...    1.0\n",
       "2   Ότι ήσουν μαλάκω το ξέραμε. Ότι ήσουν αμπαλη ...    1.0\n",
       "3  ΠΟΙΟΣ ΓΑΜΙΟΛΗΣ ΚΡΥΒΕΤΑΙ ΠΙΣΩ ΑΠΤΗΝΕ ΑΝΑΠΑΤΕΧΗ ...    1.0\n",
       "4  Βλεπω αυτόν τον πατέρα στην Σκατιανα και μου έ...    1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing  = pd.read_csv('TWEETS_nolinks.csv')\n",
    "testing  = pd.read_csv('CHATGPT TWEETS_nolinks.csv')\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "testing['class'] = testing['class'].replace(' ', np.nan)\n",
    "final = testing[testing['class'].notnull()]\n",
    "final = final.reset_index()\n",
    "final = final[['Tweet', 'class']]\n",
    "# Use astype to convert to float\n",
    "final['class'] = final['class'].astype(float)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "462a42f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Η απορία της ημέρας!!\\n\\nΓιατί σε φωτογραφία π...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ρε κόψτε την σύνδεση με τον βλακα Καραγιάννη έ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ότι ήσουν μαλάκω το ξέραμε. Ότι ήσουν αμπαλη ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ΠΟΙΟΣ ΓΑΜΙΟΛΗΣ ΚΡΥΒΕΤΑΙ ΠΙΣΩ ΑΠΤΗΝΕ ΑΝΑΠΑΤΕΧΗ ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Βλεπω αυτόν τον πατέρα στην Σκατιανα και μου έ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Μου αρέσει που σου μιλάνε στον πληθυντικό ρ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Δεν έχει άλλα λόγια για να τον χαρακτηρίσει ο ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>Αν κ δε βλέπω να μιλάς σοβαρά, θα το προσπαθή...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Όλη η #Γαλλια ειναι μια εμπολεμη ζώνη! Και όλο...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Τι σχέση έχουν αυτοί με το πατρίς θρησκεία......</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>556 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet  class\n",
       "0    Η απορία της ημέρας!!\\n\\nΓιατί σε φωτογραφία π...    0.0\n",
       "1    Ρε κόψτε την σύνδεση με τον βλακα Καραγιάννη έ...    1.0\n",
       "2     Ότι ήσουν μαλάκω το ξέραμε. Ότι ήσουν αμπαλη ...    1.0\n",
       "3    ΠΟΙΟΣ ΓΑΜΙΟΛΗΣ ΚΡΥΒΕΤΑΙ ΠΙΣΩ ΑΠΤΗΝΕ ΑΝΑΠΑΤΕΧΗ ...    1.0\n",
       "4    Βλεπω αυτόν τον πατέρα στην Σκατιανα και μου έ...    1.0\n",
       "..                                                 ...    ...\n",
       "551     Μου αρέσει που σου μιλάνε στον πληθυντικό ρ...    1.0\n",
       "552  Δεν έχει άλλα λόγια για να τον χαρακτηρίσει ο ...    1.0\n",
       "553   Αν κ δε βλέπω να μιλάς σοβαρά, θα το προσπαθή...    0.0\n",
       "554  Όλη η #Γαλλια ειναι μια εμπολεμη ζώνη! Και όλο...    0.0\n",
       "555   Τι σχέση έχουν αυτοί με το πατρίς θρησκεία......    0.0\n",
       "\n",
       "[556 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4df0d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.pie(final['Label'].value_counts().values,\n",
    "#         labels = final['Label'].value_counts().index,\n",
    "#         autopct='%1.1f%%')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf4a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(final['Label'][final['Label']==0]) #neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c279e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final['Label'][final['Label']==1]) #hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final['Label'][final['Label']==2]) #offensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a699ac6",
   "metadata": {},
   "source": [
    " PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f7345e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMultiple(main, replacements, new):\n",
    "    for elem in replacements:\n",
    "        if elem in main:\n",
    "            main = main.replace(elem, new)\n",
    "\n",
    "    return main\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    x = x.replace('ά', 'α')\n",
    "    x = x.replace('έ', 'ε')\n",
    "    x = x.replace('ή', 'η')\n",
    "    x = replaceMultiple(x, ['ί', 'ΐ', 'ϊ'], 'ι')\n",
    "    x = x.replace('ό', 'ο')\n",
    "    x = replaceMultiple(x, ['ύ', 'ΰ', 'ϋ'], 'υ')\n",
    "    x = x.replace('ώ', 'ω')\n",
    "    return x\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('greek')\n",
    " \n",
    "    imp_words = []\n",
    " \n",
    "    # Storing the important words\n",
    "    for word in str(text).split():\n",
    " \n",
    "        if word not in stop_words:\n",
    " \n",
    "            # Let's Lemmatize the word as well -- to fernw stin arxiki tou morfi\n",
    "#             before appending to the imp_words list.\n",
    " \n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatizer.lemmatize(word) #doesnt really work\n",
    " \n",
    "            imp_words.append(word)\n",
    " \n",
    "    output = \" \".join(imp_words)\n",
    " \n",
    "    return output\n",
    "\n",
    "\n",
    "punctuations_list = string.punctuation\n",
    "def remove_punctuations(text):\n",
    "    temp = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(temp)\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8dd9b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text): \n",
    "    text = normalize(text)  #diwxnw tonous\n",
    "    text = text.lower() #ola mikra\n",
    "    text = remove_punctuations(text) #diwxnw punctiations\n",
    "    text = remove_stopwords(text) #diwxnw stopwords\n",
    "    text  = \" \".join([w.lemma_ for w in nlp(text)]) #lemmatization is not really working well either\n",
    "    return text\n",
    "#     return [word for word in text.split() ] #tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec88fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Tweet'] = final['Tweet'].apply(lambda x: text_processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32be0385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>αποριο ημερα γιατι φωτογραφια ανεβαζω ενας γυν...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ρε κοψτε συνδεση βλακα καραγιαννη ελεοςς εχω θ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ησω μαλακω ξερω ησω αμπαλος ξερω υπανθρωπος απ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ποιος γαμιολη κρυβω πισω απτηνε αναπατεχης αδι...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>βλεπω αυτος πας σκατιανα μου ερχω μυαλος αυτος...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  class\n",
       "0  αποριο ημερα γιατι φωτογραφια ανεβαζω ενας γυν...    0.0\n",
       "1  ρε κοψτε συνδεση βλακα καραγιαννη ελεοςς εχω θ...    1.0\n",
       "2  ησω μαλακω ξερω ησω αμπαλος ξερω υπανθρωπος απ...    1.0\n",
       "3  ποιος γαμιολη κρυβω πισω απτηνε αναπατεχης αδι...    1.0\n",
       "4  βλεπω αυτος πας σκατιανα μου ερχω μυαλος αυτος...    1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef7ab904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "OGTD  = pd.read_csv('OGTDv1.csv')\n",
    "OGTD = OGTD[['Tweet', 'Label']]\n",
    "OGTD\n",
    "\n",
    "OGTD['Label'] = OGTD['Label'].replace('Offensive', 1)\n",
    "OGTD['Label'] = OGTD['Label'].replace('Not Offensive', 0)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    hashtags = \"#[\\S]+\"\n",
    "    mentions = \"@[\\S]+\"\n",
    "    url = \"https?://[A-z0-9_%/\\-\\.]+[A-z0-9_\\.\\-\\?&=%]+\"\n",
    "    text = re.sub(url,\"\",text)\n",
    "    text = re.sub(hashtags,\"\",text)\n",
    "    text = re.sub(mentions,\"\",text)\n",
    "    puntuations = \"[\\.\\?!,;:]+\"\n",
    "    text = re.sub(puntuations,\"\",text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def text_processing_OGTD(text):\n",
    "    text = normalize(text)  #diwxnw tonous\n",
    "    text = text.lower() #ola mikra\n",
    "    text = remove_punctuations(text) #diwxnw punctiations\n",
    "#     text = remove_stopwords(text) #diwxnw stopwords\n",
    "#     text  = \" \".join([w.lemma_ for w in nlp(text)]) #lemmatization is not really working well either\n",
    "    return text\n",
    "\n",
    "OGTD['Tweet'] = OGTD['Tweet'].apply(lambda x: text_processing_OGTD(x))\n",
    "OGTD['Tweet'] = OGTD['Tweet'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee28a665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user που εισαι να σε ξεμπερδεψω</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user και εμενα τα ιδια μου λενε νεα εισαι εχει...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>σχολιαστες για τις εκλογες σημερα… ντανος ζαχα...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user σε ευχαριστω πολυ φιλαρακι μου να εισαι καλα</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>εντωμεταξυ οι ανθρωποι ποσο για τον πουτσο</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>την φαγανε τη γυναικα shoppingstar</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>εχει ωραια γευση λεει ροκφορ λεει ο χαρισο εγω...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <td>ο βασιλης απο τι κουραστηκε ακριβως poweroflovegr</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>καλοταξιδο κουκλαρα μας user skaipolxeftiles p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>θεσσαλονικη θα πατε κοριτσια θα αλλαζετε καθε ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5335 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet  class\n",
       "0                       user που εισαι να σε ξεμπερδεψω    0.0\n",
       "1     user και εμενα τα ιδια μου λενε νεα εισαι εχει...    0.0\n",
       "2     σχολιαστες για τις εκλογες σημερα… ντανος ζαχα...    0.0\n",
       "3     user σε ευχαριστω πολυ φιλαρακι μου να εισαι καλα    0.0\n",
       "4            εντωμεταξυ οι ανθρωποι ποσο για τον πουτσο    1.0\n",
       "...                                                 ...    ...\n",
       "5330                 την φαγανε τη γυναικα shoppingstar    0.0\n",
       "5331  εχει ωραια γευση λεει ροκφορ λεει ο χαρισο εγω...    0.0\n",
       "5332  ο βασιλης απο τι κουραστηκε ακριβως poweroflovegr    0.0\n",
       "5333  καλοταξιδο κουκλαρα μας user skaipolxeftiles p...    0.0\n",
       "5334  θεσσαλονικη θα πατε κοριτσια θα αλλαζετε καθε ...    0.0\n",
       "\n",
       "[5335 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming column 'A' to 'New_Column'\n",
    "OGTD = OGTD.rename(columns={'Label': 'class'}) #change the name so i can merge the 2 datasets\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "final_merged = pd.concat([final, OGTD])\n",
    "final_merged = final_merged.sample(frac=1) #SHUFFLE\n",
    "final_merged.reset_index(inplace=True)\n",
    "final_merged = final_merged.drop('index', axis=1)\n",
    "final_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71477220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81ae5ae0",
   "metadata": {},
   "source": [
    "# TF-IDF unigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f9f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5140\n"
     ]
    }
   ],
   "source": [
    "Tfidf_vect = TfidfVectorizer() \n",
    "\n",
    "Tfidf_vect.fit(final['Tweet'])\n",
    "final_tdidf = Tfidf_vect.transform(final['Tweet']) \n",
    "\n",
    "print(len(Tfidf_vect.vocabulary_)) \n",
    "#decide that many cause after was over analysing and \n",
    "# separating wrongly words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4533430",
   "metadata": {},
   "source": [
    "split data it into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdcd87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=10, shuffle=True)\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(final_tdidf, final['class'])):\n",
    "        X_train, X_test, Y_train, Y_test = final_tdidf[train_index], final_tdidf[test_index], final['class'][train_index], final['class'][test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de54303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALLOS TROPOS ANTI NA KANW TO STRATIFIED!!!!\n",
    "\n",
    "# xtrain, xvalid, ytrain, yvalid = train_test_split(        \n",
    "#   final.Tweet.values,final.Label.values,stratify=final.Label.values, \n",
    "#   random_state=42, \n",
    "#   test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee65fb05",
   "metadata": {},
   "source": [
    "\n",
    "<!-- # xtrain, xvalid, ytrain, yvalid = train_test_split(final.Tweet.values, final.Label.values, \n",
    "#                                                   stratify=final.Label.values, \n",
    "#                                                   random_state=42, \n",
    "#                                                   test_size=0.2, shuffle=True) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb7ff0",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d46442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ................C=1000, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ................C=1000, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ................C=1000, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "{'kernel': 'rbf', 'gamma': 1, 'C': 100}\n",
      "SVC(C=100, class_weight='balanced', gamma=1)\n"
     ]
    }
   ],
   "source": [
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 'scale'],\n",
    "              'kernel': ['rbf', 'linear']} \n",
    "\n",
    "# grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)\n",
    "grid = RandomizedSearchCV(svm.SVC(class_weight='balanced'),  param_grid, cv=3, random_state=42, refit = True, verbose = 2)\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, Y_train)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb7fe546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.89      0.86        57\n",
      "         1.0       0.88      0.81      0.85        54\n",
      "\n",
      "    accuracy                           0.86       111\n",
      "   macro avg       0.86      0.85      0.86       111\n",
      "weighted avg       0.86      0.86      0.86       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(Y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a0f03",
   "metadata": {},
   "source": [
    "# Multinomial Naïve Bayes (works with occurrence counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d68df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  88.28828828828829\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train,Y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(X_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb8d6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.89      0.89        57\n",
      "         1.0       0.89      0.87      0.88        54\n",
      "\n",
      "    accuracy                           0.88       111\n",
      "   macro avg       0.88      0.88      0.88       111\n",
      "weighted avg       0.88      0.88      0.88       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "print(classification_report(Y_test, predictions_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd4573",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668abd80",
   "metadata": {},
   "source": [
    "* n_estimators = number of trees in the forest\n",
    "* max_features = max number of features considered for splitting a node\n",
    "* max_depth = max number of levels in each decision tree\n",
    "* min_samples_split = min number of data points placed in a node before the node is split\n",
    "* min_samples_leaf = min number of data points allowed in a leaf node\n",
    "* bootstrap = method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857887d5",
   "metadata": {},
   "source": [
    "https://medium.com/sfu-cspmp/surviving-in-a-random-forest-with-imbalanced-datasets-b98b963d52eb - den to eida olo\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b2364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 300, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion': ['gini', 'entropy'],\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e9fb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=100, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.7s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=100, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.6s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=100, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.6s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=60, min_samples_leaf=4, min_samples_split=10, n_estimators=222; total time=   0.4s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=60, min_samples_leaf=4, min_samples_split=10, n_estimators=222; total time=   0.4s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=60, min_samples_leaf=4, min_samples_split=10, n_estimators=222; total time=   0.4s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=110, min_samples_leaf=2, min_samples_split=2, n_estimators=222; total time=   0.9s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=110, min_samples_leaf=2, min_samples_split=2, n_estimators=222; total time=   0.9s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=110, min_samples_leaf=2, min_samples_split=2, n_estimators=222; total time=   1.0s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=60, min_samples_leaf=1, min_samples_split=5, n_estimators=266; total time=   1.3s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=60, min_samples_leaf=1, min_samples_split=5, n_estimators=266; total time=   1.3s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=60, min_samples_leaf=1, min_samples_split=5, n_estimators=266; total time=   1.4s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=244; total time=   1.0s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=244; total time=   0.9s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=244; total time=   1.0s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=211; total time=   1.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=211; total time=   1.5s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=211; total time=   1.4s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.4s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   1.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.7s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=70, min_samples_leaf=4, min_samples_split=2, n_estimators=255; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=70, min_samples_leaf=4, min_samples_split=2, n_estimators=255; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=70, min_samples_leaf=4, min_samples_split=2, n_estimators=255; total time=   0.4s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.9s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.9s\n",
      "{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 20, 'criterion': 'gini', 'bootstrap': True}\n",
      "RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
      "                       min_samples_leaf=2, min_samples_split=5,\n",
      "                       n_estimators=200, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier(random_state =0, class_weight='balanced')\n",
    "\n",
    "grid = RandomizedSearchCV(RandomForest, random_grid, cv=3, random_state=42, refit = True, verbose = 2)\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, Y_train)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e74b6619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.91      0.90        57\n",
      "         1.0       0.90      0.87      0.89        54\n",
      "\n",
      "    accuracy                           0.89       111\n",
      "   macro avg       0.89      0.89      0.89       111\n",
      "weighted avg       0.89      0.89      0.89       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(Y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d51d81",
   "metadata": {},
   "source": [
    "# SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1883889",
   "metadata": {},
   "source": [
    "https://michael-fuchs-python.netlify.app/2019/11/11/introduction-to-sgd-classifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0f89acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# SGDC = SGDClassifier(class_weight='balanced')\n",
    "# SGDC.fit(X_train, Y_train)\n",
    "# prediction_SGDC = SGDC.predict(X_test)\n",
    "\n",
    "# # print classification report\n",
    "# print(classification_report(Y_test, prediction_SGDC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8bd4df0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END alpha=0.001, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time=   0.0s\n",
      "[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time=   0.0s\n",
      "[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time=   0.0s\n",
      "[CV] END ..alpha=0.0001, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.0001, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.0001, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.1, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.1, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.1, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.0s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.0s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.0s\n",
      "[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   0.0s\n",
      "[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   0.0s\n",
      "[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   0.0s\n",
      "{'penalty': 'l2', 'loss': 'log', 'alpha': 0.0001}\n",
      "SGDClassifier(class_weight='balanced', loss='log')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SGDC = SGDClassifier(class_weight='balanced')\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = { \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\", \"perceptron\"],\n",
    "                \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "                \"penalty\" : [\"l2\", \"l1\", \"elasticnet\", \"none\"]}\n",
    "\n",
    "grid = RandomizedSearchCV(SGDC, random_grid, cv=3, random_state=42, refit = True, verbose = 2)\n",
    "\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, Y_train)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1f4794d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.89      0.88        57\n",
      "         1.0       0.88      0.85      0.87        54\n",
      "\n",
      "    accuracy                           0.87       111\n",
      "   macro avg       0.87      0.87      0.87       111\n",
      "weighted avg       0.87      0.87      0.87       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(Y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87871b",
   "metadata": {},
   "source": [
    "# XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "651f92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb90a85",
   "metadata": {},
   "source": [
    "sample_weights: \n",
    "    https://stackoverflow.com/questions/67868420/xgboost-for-multiclassification-and-imbalanced-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9746f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82        58\n",
      "           1       0.80      0.81      0.80        53\n",
      "\n",
      "    accuracy                           0.81       111\n",
      "   macro avg       0.81      0.81      0.81       111\n",
      "weighted avg       0.81      0.81      0.81       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=Y_train \n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=1000, nthread= 3).fit(X_train, Y_train, sample_weight=sample_weights) \n",
    "prediction_xgb = xgb.predict(X_test)\n",
    "# f1_score(Y_test, prediction_xgb)\n",
    "\n",
    "print(classification_report(prediction_xgb,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da0c4852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=500; total time=   0.6s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=500; total time=   0.7s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=500; total time=   0.8s\n",
      "[CV] END gamma=0.3, learning_rate=0.15, max_depth=6, min_child_weight=7, n_estimators=1500; total time=   2.4s\n",
      "[CV] END gamma=0.3, learning_rate=0.15, max_depth=6, min_child_weight=7, n_estimators=1500; total time=   1.8s\n",
      "[CV] END gamma=0.3, learning_rate=0.15, max_depth=6, min_child_weight=7, n_estimators=1500; total time=   1.9s\n",
      "[CV] END gamma=0.2, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=100; total time=   0.2s\n",
      "[CV] END gamma=0.2, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=100; total time=   0.2s\n",
      "[CV] END gamma=0.2, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=100; total time=   0.2s\n",
      "[CV] END gamma=0.3, learning_rate=0.05, max_depth=15, min_child_weight=5, n_estimators=1500; total time=   2.2s\n",
      "[CV] END gamma=0.3, learning_rate=0.05, max_depth=15, min_child_weight=5, n_estimators=1500; total time=   2.3s\n",
      "[CV] END gamma=0.3, learning_rate=0.05, max_depth=15, min_child_weight=5, n_estimators=1500; total time=   2.2s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=15, min_child_weight=7, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=15, min_child_weight=7, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=15, min_child_weight=7, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.4, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=1500; total time=   2.0s\n",
      "[CV] END gamma=0.4, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=1500; total time=   2.2s\n",
      "[CV] END gamma=0.4, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=1500; total time=   2.2s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=6, min_child_weight=1, n_estimators=1500; total time=   2.7s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=6, min_child_weight=1, n_estimators=1500; total time=   2.5s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=6, min_child_weight=1, n_estimators=1500; total time=   2.4s\n",
      "[CV] END gamma=0.4, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=1100; total time=   1.2s\n",
      "[CV] END gamma=0.4, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=1100; total time=   0.9s\n",
      "[CV] END gamma=0.4, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=1100; total time=   1.4s\n",
      "[CV] END gamma=0.0, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500; total time=   0.7s\n",
      "[CV] END gamma=0.0, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500; total time=   0.6s\n",
      "[CV] END gamma=0.0, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500; total time=   0.6s\n",
      "{'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.05, 'gamma': 0.2}\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = { \"learning_rate\" : [0.05,0.1,0.15,0.20],\n",
    "                 \"n_estimators\" : [100, 500, 900, 1100, 1500],\n",
    "                 \"max_depth\" : [ 3, 5, 6, 10, 15],\n",
    "                 \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "                 \"gamma\" : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ]}\n",
    "\n",
    "grid = RandomizedSearchCV(xgb, random_grid, cv=3, random_state=42, refit = True, verbose = 2)\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, Y_train, sample_weight=sample_weights)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ef72309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.95      0.89        57\n",
      "         1.0       0.93      0.80      0.86        54\n",
      "\n",
      "    accuracy                           0.87       111\n",
      "   macro avg       0.88      0.87      0.87       111\n",
      "weighted avg       0.88      0.87      0.87       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(Y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44f9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a37a6226",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a57b07e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'αντε ρε μαλακα'\n",
    "sentence = text_processing(sentence)\n",
    "sentence = Tfidf_vect.transform([sentence]) \n",
    "prediction = Naive.predict(sentence)\n",
    "prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221f856",
   "metadata": {},
   "source": [
    "# TRAIN MODELS WITH THE MERGED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d4f7868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5335, 2)\n",
      "(4779, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape to ensure the expected structure\n",
    "print(final_merged.shape)\n",
    "print(OGTD.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "728907f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_merged, X_val_merged, y_train_merged, y_val_merged = train_test_split(final_merged['Tweet'], final_merged['class'], test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(final['Tweet'], final['class'], test_size=0.2, random_state=42)\n",
    "\n",
    "# test it on OGTD dataset\n",
    "X_train_GREEK_2, X_val_GREEK_2, y_train_GREEK_2, y_val_GREEK_2 = train_test_split(OGTD['Tweet'], OGTD['class'], test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7bd7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # κραταμε μονο τα κοινα μερτζεεντ\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# def remove_common_values(series_a, series_b):\n",
    "#     common_values = set(series_a).intersection(set(series_b))\n",
    "#     # Remove rows with common values from series_a\n",
    "#     series_a = series_a[~series_a.isin(common_values)]\n",
    "#     return series_a\n",
    "# modified_series_a = remove_common_values(X_train_merged, X_val)\n",
    "\n",
    "\n",
    "# def keep_common_indices(series1, series2):\n",
    "#     common_indices = series1.index.intersection(series2.index)\n",
    "#     series1_common = series1.loc[common_indices]\n",
    "#     series2_common = series2.loc[common_indices]\n",
    "#     return series1_common, series2_common\n",
    "\n",
    "\n",
    "# common_series_a, common_series_b = keep_common_indices(modified_series_a, y_train_merged) #merged gia ta ellhnika\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba646836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_common_values(series_a, series_b):\n",
    "    common_values = set(series_a).intersection(set(series_b))\n",
    "    # Remove rows with common values from series_a\n",
    "    series_a = series_a[~series_a.isin(common_values)]\n",
    "    return series_a\n",
    "\n",
    "modified_series_OGTD = remove_common_values(X_train_merged, X_val_GREEK_2)\n",
    "\n",
    "# print(\"Modified series_a:\")\n",
    "# print(modified_series_a)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def keep_common_indices(series1, series2):\n",
    "    common_indices = series1.index.intersection(series2.index)\n",
    "    series1_common = series1.loc[common_indices]\n",
    "    series2_common = series2.loc[common_indices]\n",
    "    return series1_common, series2_common\n",
    "\n",
    "\n",
    "common_series_X_OGTD, common_series_Y_OGTD = keep_common_indices(modified_series_OGTD, y_train_merged) #merged gia OGTD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c44f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "331897d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "Tfidf_vect = TfidfVectorizer() \n",
    "Tfidf_vect.fit(X_train_merged)\n",
    "\n",
    "common_series_X_OGTD_tfidf = Tfidf_vect.transform(common_series_X_OGTD) \n",
    "# common_series_a_tfidf = Tfidf_vect.transform(common_series_a) \n",
    "\n",
    "OGTD_tfidf = Tfidf_vect.transform(X_val_GREEK_2) \n",
    "# final_tfidf = Tfidf_vect.transform(X_val) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dba90e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM\n",
    "# Tfidf_vect = TfidfVectorizer() \n",
    "\n",
    "# Tfidf_vect.fit(X_train_merged)\n",
    "# final_merged_tfidf = Tfidf_vect.transform(X_train_merged) \n",
    "\n",
    "\n",
    "\n",
    "# OGTD_tfidf = Tfidf_vect.transform(OGTD['Tweet']) \n",
    "# final_tfidf = Tfidf_vect.transform(X_val['Tweet']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da642411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_merged, X_val_merged, y_train_merged, y_val_merged = train_test_split(final_merged_tfidf, final_merged['class'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # test it on OGTD dataset\n",
    "# X_train_GREEK_2, X_val_GREEK_2, y_train_GREEK_2, y_val_GREEK_2 = train_test_split(OGTD_tfidf, OGTD['class'], test_size=0.3, random_state=42)\n",
    "\n",
    "# # test it on MY dataset\n",
    "# X_train, X_val, y_train, y_val = train_test_split(final_tfidf, final['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367fe6d8",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4c647",
   "metadata": {},
   "source": [
    "SVM ON MINE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26d75fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, class_weight=&#x27;balanced&#x27;, gamma=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100, class_weight=&#x27;balanced&#x27;, gamma=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=100, class_weight='balanced', gamma=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = svm.SVC(C=100, class_weight='balanced', gamma=1, kernel='rbf')\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(common_series_a_tfidf, common_series_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4c11f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      1.00      0.76        44\n",
      "         1.0       1.00      0.59      0.74        68\n",
      "\n",
      "    accuracy                           0.75       112\n",
      "   macro avg       0.81      0.79      0.75       112\n",
      "weighted avg       0.85      0.75      0.75       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(final_tfidf)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_val, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127942d",
   "metadata": {},
   "source": [
    "ON OGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "24aa76ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88       988\n",
      "           1       0.92      0.45      0.60       446\n",
      "\n",
      "    accuracy                           0.82      1434\n",
      "   macro avg       0.86      0.71      0.74      1434\n",
      "weighted avg       0.84      0.82      0.79      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid = svm.SVC(C=100, class_weight='balanced', gamma=1, kernel='rbf')\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(common_series_X_OGTD_tfidf, common_series_Y_OGTD)\n",
    "\n",
    "grid_predictions = grid.predict(OGTD_tfidf)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_val_GREEK_2, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b23de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aff6070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(class_weight=&#x27;balanced&#x27;, loss=&#x27;log&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(class_weight=&#x27;balanced&#x27;, loss=&#x27;log&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(class_weight='balanced', loss='log')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "grid = SGDClassifier(class_weight='balanced', loss='log', penalty= 'l2',  alpha= 0.0001)\n",
    "\n",
    "grid.fit(common_series_a_tfidf, common_series_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97f7e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.91      0.89        44\n",
      "         1.0       0.94      0.91      0.93        68\n",
      "\n",
      "    accuracy                           0.91       112\n",
      "   macro avg       0.90      0.91      0.91       112\n",
      "weighted avg       0.91      0.91      0.91       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(final_tfidf)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_val, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8337491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5533d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69ef4bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       988\n",
      "           1       0.77      0.64      0.70       446\n",
      "\n",
      "    accuracy                           0.83      1434\n",
      "   macro avg       0.81      0.78      0.79      1434\n",
      "weighted avg       0.83      0.83      0.82      1434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "grid = SGDClassifier(class_weight='balanced', loss='log', penalty= 'l2',  alpha= 0.0001)\n",
    "\n",
    "grid.fit(common_series_X_OGTD_tfidf, common_series_Y_OGTD)\n",
    "\n",
    "grid_predictions = grid.predict(OGTD_tfidf)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_val_GREEK_2, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f78337",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "986c44e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(common_series_a_tfidf, common_series_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad069eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      1.00      0.64        44\n",
      "         1.0       1.00      0.28      0.44        68\n",
      "\n",
      "    accuracy                           0.56       112\n",
      "   macro avg       0.74      0.64      0.54       112\n",
      "weighted avg       0.79      0.56      0.52       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(final_tfidf)\n",
    "\n",
    "print(classification_report(y_val, predictions_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4aa6ac61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82       988\n",
      "           1       0.92      0.05      0.10       446\n",
      "\n",
      "    accuracy                           0.70      1434\n",
      "   macro avg       0.81      0.52      0.46      1434\n",
      "weighted avg       0.77      0.70      0.60      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(common_series_X_OGTD_tfidf, common_series_Y_OGTD)\n",
    "\n",
    "\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(OGTD_tfidf)\n",
    "print(classification_report(y_val_GREEK_2, predictions_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d21f0fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=20,\n",
       "                       min_samples_leaf=2, min_samples_split=5,\n",
       "                       n_estimators=200, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=20,\n",
       "                       min_samples_leaf=2, min_samples_split=5,\n",
       "                       n_estimators=200, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
       "                       min_samples_leaf=2, min_samples_split=5,\n",
       "                       n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "grid = RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
    "                       min_samples_leaf=2, min_samples_split=5,\n",
    "                       n_estimators=200, random_state=0, bootstrap= True, criterion='gini')\n",
    "# fitting the model for grid search\n",
    "grid.fit(common_series_a_tfidf, common_series_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dc13c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.89      0.81        44\n",
      "         1.0       0.92      0.81      0.86        68\n",
      "\n",
      "    accuracy                           0.84       112\n",
      "   macro avg       0.83      0.85      0.84       112\n",
      "weighted avg       0.85      0.84      0.84       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(final_tfidf)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_val, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d33dbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88       988\n",
      "           1       0.82      0.54      0.65       446\n",
      "\n",
      "    accuracy                           0.82      1434\n",
      "   macro avg       0.82      0.74      0.77      1434\n",
      "weighted avg       0.82      0.82      0.81      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "grid = RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
    "                       min_samples_leaf=2, min_samples_split=5,\n",
    "                       n_estimators=200, random_state=0, bootstrap= True, criterion='gini')\n",
    "# fitting the model for grid search\n",
    "grid.fit(common_series_X_OGTD_tfidf, common_series_Y_OGTD)\n",
    "\n",
    "grid_predictions = grid.predict(OGTD_tfidf)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_val_GREEK_2, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6484ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = { \"learning_rate\" : [0.05,0.1,0.15,0.20],\n",
    "                 \"n_estimators\" : [100, 500, 900, 1100, 1500],\n",
    "                 \"max_depth\" : [ 3, 5, 6, 10, 15],\n",
    "                 \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "                 \"gamma\" : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ]}\n",
    "\n",
    "grid = XGBClassifier(n_estimators= 100, min_child_weight= 1, max_depth= 10,learning_rate= 0.05, gamma= 0.2)\n",
    "\n",
    "grid.fit(common_series_a_tfidf, common_series_b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a5eedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.98      0.74        44\n",
      "         1.0       0.97      0.56      0.71        68\n",
      "\n",
      "    accuracy                           0.72       112\n",
      "   macro avg       0.78      0.77      0.72       112\n",
      "weighted avg       0.82      0.72      0.72       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(final_tfidf)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_val, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d56aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29215572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "981dc51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89       988\n",
      "           1       0.89      0.53      0.67       446\n",
      "\n",
      "    accuracy                           0.83      1434\n",
      "   macro avg       0.86      0.75      0.78      1434\n",
      "weighted avg       0.84      0.83      0.82      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = { \"learning_rate\" : [0.05,0.1,0.15,0.20],\n",
    "                 \"n_estimators\" : [100, 500, 900, 1100, 1500],\n",
    "                 \"max_depth\" : [ 3, 5, 6, 10, 15],\n",
    "                 \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "                 \"gamma\" : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ]}\n",
    "\n",
    "grid = XGBClassifier(n_estimators= 100, min_child_weight= 1, max_depth= 10,learning_rate= 0.05, gamma= 0.2)\n",
    "\n",
    "grid.fit(common_series_X_OGTD_tfidf, common_series_Y_OGTD)\n",
    "\n",
    "grid_predictions = grid.predict(OGTD_tfidf)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(y_val_GREEK_2, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77971b59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
