{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce58620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Η απορία της ημέρας!!\\n\\nΓιατί σε φωτογραφία π...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ρε κόψτε την σύνδεση με τον βλακα Καραγιάννη έ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ότι ήσουν μαλάκω το ξέραμε. Ότι ήσουν αμπαλη ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ΠΟΙΟΣ ΓΑΜΙΟΛΗΣ ΚΡΥΒΕΤΑΙ ΠΙΣΩ ΑΠΤΗΝΕ ΑΝΑΠΑΤΕΧΗ ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Βλεπω αυτόν τον πατέρα στην Σκατιανα και μου έ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  class\n",
       "0  Η απορία της ημέρας!!\\n\\nΓιατί σε φωτογραφία π...    0.0\n",
       "1  Ρε κόψτε την σύνδεση με τον βλακα Καραγιάννη έ...    1.0\n",
       "2   Ότι ήσουν μαλάκω το ξέραμε. Ότι ήσουν αμπαλη ...    1.0\n",
       "3  ΠΟΙΟΣ ΓΑΜΙΟΛΗΣ ΚΡΥΒΕΤΑΙ ΠΙΣΩ ΑΠΤΗΝΕ ΑΝΑΠΑΤΕΧΗ ...    1.0\n",
       "4  Βλεπω αυτόν τον πατέρα στην Σκατιανα και μου έ...    1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PRE-PROCESSING\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# TD-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TRAIN/TEST SET\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "nlp = spacy.load(\"el_core_news_sm\")\n",
    "\n",
    "# testing  = pd.read_csv('TWEETS_nolinks.csv')\n",
    "testing  = pd.read_csv('CHATGPT TWEETS_nolinks.csv')\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "testing['class'] = testing['class'].replace(' ', np.nan)\n",
    "final = testing[testing['class'].notnull()]\n",
    "final = final.reset_index()\n",
    "final = final[['Tweet', 'class']]\n",
    "# Use astype to convert to float\n",
    "final['class'] = final['class'].astype(float)\n",
    "final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5afb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMultiple(main, replacements, new):\n",
    "    for elem in replacements:\n",
    "        if elem in main:\n",
    "            main = main.replace(elem, new)\n",
    "\n",
    "    return main\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    x = x.replace('ά', 'α')\n",
    "    x = x.replace('έ', 'ε')\n",
    "    x = x.replace('ή', 'η')\n",
    "    x = replaceMultiple(x, ['ί', 'ΐ', 'ϊ'], 'ι')\n",
    "    x = x.replace('ό', 'ο')\n",
    "    x = replaceMultiple(x, ['ύ', 'ΰ', 'ϋ'], 'υ')\n",
    "    x = x.replace('ώ', 'ω')\n",
    "    return x\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('greek')\n",
    " \n",
    "    imp_words = []\n",
    " \n",
    "    # Storing the important words\n",
    "    for word in str(text).split():\n",
    " \n",
    "        if word not in stop_words:\n",
    " \n",
    "            # Let's Lemmatize the word as well -- to fernw stin arxiki tou morfi\n",
    "#             before appending to the imp_words list.\n",
    " \n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatizer.lemmatize(word) #doesnt really work\n",
    " \n",
    "            imp_words.append(word)\n",
    " \n",
    "    output = \" \".join(imp_words)\n",
    " \n",
    "    return output\n",
    "\n",
    "\n",
    "punctuations_list = string.punctuation\n",
    "def remove_punctuations(text):\n",
    "    temp = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(temp)\n",
    "\n",
    "def text_processing(text): \n",
    "    text = normalize(text)  #diwxnw tonous\n",
    "    text = text.lower() #ola mikra\n",
    "    text = remove_punctuations(text) #diwxnw punctiations\n",
    "    text = remove_stopwords(text) #diwxnw stopwords\n",
    "#     text  = \" \".join([w.lemma_ for w in nlp(text)]) #lemmatization is not really working well either\n",
    "    return text\n",
    "#     return [word for word in text.split() ] #tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5474a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Tweet'] = final['Tweet'].apply(lambda x: text_processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b342a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9665\n"
     ]
    }
   ],
   "source": [
    "Tfidf_vect = TfidfVectorizer(ngram_range = (2, 2)) \n",
    "#maximum of 2000 unique words/features as we have set parameter max_features=2000\n",
    "final_tdidf = Tfidf_vect.fit_transform(final['Tweet'])\n",
    "\n",
    "print(len(Tfidf_vect.vocabulary_)) \n",
    "#decide that many cause after was over analysing and \n",
    "# separating wrongly words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0997f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=10, shuffle=True)\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(final_tdidf, final['class'])):\n",
    "        X_train, X_test, Y_train, Y_test = final_tdidf[train_index], final_tdidf[test_index], final['class'][train_index], final['class'][test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221bc6a0",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e44dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.01, kernel=linear; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END .................C=100, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ................C=1000, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ................C=1000, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "[CV] END ................C=1000, gamma=0.0001, kernel=linear; total time=   0.0s\n",
      "{'kernel': 'linear', 'gamma': 0.0001, 'C': 100}\n",
      "SVC(C=100, class_weight='balanced', gamma=0.0001, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 'scale'],\n",
    "              'kernel': ['rbf', 'linear']} \n",
    "\n",
    "# grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)\n",
    "grid = RandomizedSearchCV(svm.SVC(class_weight='balanced'),  param_grid, cv=3, random_state=42, refit = True, verbose = 2)\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, Y_train)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caeb9460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.96      0.77        57\n",
      "         1.0       0.92      0.44      0.60        54\n",
      "\n",
      "    accuracy                           0.71       111\n",
      "   macro avg       0.79      0.70      0.69       111\n",
      "weighted avg       0.78      0.71      0.69       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(Y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a56ea34",
   "metadata": {},
   "source": [
    "# Multinomial Naïve Bayes (works with occurrence counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09046611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  70.27027027027027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.93      0.76        57\n",
      "         1.0       0.86      0.46      0.60        54\n",
      "\n",
      "    accuracy                           0.70       111\n",
      "   macro avg       0.75      0.70      0.68       111\n",
      "weighted avg       0.75      0.70      0.68       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train,Y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(X_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Y_test)*100)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(Y_test, predictions_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370fb51",
   "metadata": {},
   "source": [
    "Multinomial Naïve Bayes (works with occurrence counts) - 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea6f0715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      1.00      0.82       584\n",
      "         1.0       0.50      0.00      0.01       253\n",
      "\n",
      "    accuracy                           0.70       837\n",
      "   macro avg       0.60      0.50      0.41       837\n",
      "weighted avg       0.64      0.70      0.58       837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train,Y_train_2)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(X_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "# print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Y_test_2)*100)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(Y_test_2, predictions_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7f3ca",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4e6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=100, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=100, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=100, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=60, min_samples_leaf=4, min_samples_split=10, n_estimators=222; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=60, min_samples_leaf=4, min_samples_split=10, n_estimators=222; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=60, min_samples_leaf=4, min_samples_split=10, n_estimators=222; total time=   0.2s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=110, min_samples_leaf=2, min_samples_split=2, n_estimators=222; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=110, min_samples_leaf=2, min_samples_split=2, n_estimators=222; total time=   0.3s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=110, min_samples_leaf=2, min_samples_split=2, n_estimators=222; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=60, min_samples_leaf=1, min_samples_split=5, n_estimators=266; total time=   1.9s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=60, min_samples_leaf=1, min_samples_split=5, n_estimators=266; total time=   1.8s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=60, min_samples_leaf=1, min_samples_split=5, n_estimators=266; total time=   1.8s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=244; total time=   1.3s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=244; total time=   2.1s\n",
      "[CV] END bootstrap=False, criterion=entropy, max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=244; total time=   3.0s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=211; total time=   5.4s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=211; total time=   4.4s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=211; total time=   5.1s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV] END bootstrap=False, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=70, min_samples_leaf=4, min_samples_split=2, n_estimators=255; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=70, min_samples_leaf=4, min_samples_split=2, n_estimators=255; total time=   0.4s\n",
      "[CV] END bootstrap=True, criterion=entropy, max_depth=70, min_samples_leaf=4, min_samples_split=2, n_estimators=255; total time=   0.3s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.6s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.6s\n",
      "[CV] END bootstrap=True, criterion=gini, max_depth=90, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   0.6s\n",
      "{'n_estimators': 244, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 30, 'criterion': 'entropy', 'bootstrap': False}\n",
      "RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
      "                       criterion='entropy', max_depth=30, min_samples_split=10,\n",
      "                       n_estimators=244, random_state=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      1.00      0.75        57\n",
      "         1.0       1.00      0.28      0.43        54\n",
      "\n",
      "    accuracy                           0.65       111\n",
      "   macro avg       0.80      0.64      0.59       111\n",
      "weighted avg       0.79      0.65      0.59       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining parameter range\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 300, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion': ['gini', 'entropy'],\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier(random_state =0, class_weight='balanced')\n",
    "\n",
    "grid = RandomizedSearchCV(RandomForest, random_grid, cv=3, random_state=42, refit = True, verbose = 2)\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, Y_train)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "grid_predictions = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(Y_test, grid_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab23e8a",
   "metadata": {},
   "source": [
    "# SGDClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5348a304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END alpha=0.001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time=   0.0s\n",
      "[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time=   0.0s\n",
      "[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time=   0.0s\n",
      "[CV] END ..alpha=0.0001, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.0001, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.0001, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.1, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.1, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END ..alpha=0.1, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.0s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.0s\n",
      "[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   0.0s\n",
      "[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   0.0s\n",
      "[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   0.0s\n",
      "[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   0.0s\n",
      "{'penalty': 'l2', 'loss': 'log', 'alpha': 0.0001}\n",
      "SGDClassifier(class_weight='balanced', loss='log')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.91      0.75        57\n",
      "         1.0       0.83      0.46      0.60        54\n",
      "\n",
      "    accuracy                           0.69       111\n",
      "   macro avg       0.74      0.69      0.67       111\n",
      "weighted avg       0.74      0.69      0.68       111\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\konst\\anaconda3\\envs\\udemy\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SGDC = SGDClassifier(class_weight='balanced')\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = { \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\", \"perceptron\"],\n",
    "                \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "                \"penalty\" : [\"l2\", \"l1\", \"elasticnet\", \"none\"]}\n",
    "\n",
    "grid = RandomizedSearchCV(SGDC, random_grid, cv=3, random_state=42, refit = True, verbose = 2)\n",
    "\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, Y_train)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "grid_predictions = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(Y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0355a",
   "metadata": {},
   "source": [
    "\n",
    "# XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01714532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=500; total time=   0.6s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=500; total time=   0.2s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=500; total time=   0.3s\n",
      "[CV] END gamma=0.3, learning_rate=0.15, max_depth=6, min_child_weight=7, n_estimators=1500; total time=   1.0s\n",
      "[CV] END gamma=0.3, learning_rate=0.15, max_depth=6, min_child_weight=7, n_estimators=1500; total time=   0.9s\n",
      "[CV] END gamma=0.3, learning_rate=0.15, max_depth=6, min_child_weight=7, n_estimators=1500; total time=   1.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.3, learning_rate=0.05, max_depth=15, min_child_weight=5, n_estimators=1500; total time=   1.0s\n",
      "[CV] END gamma=0.3, learning_rate=0.05, max_depth=15, min_child_weight=5, n_estimators=1500; total time=   0.9s\n",
      "[CV] END gamma=0.3, learning_rate=0.05, max_depth=15, min_child_weight=5, n_estimators=1500; total time=   0.9s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=15, min_child_weight=7, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=15, min_child_weight=7, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=15, min_child_weight=7, n_estimators=100; total time=   0.0s\n",
      "[CV] END gamma=0.4, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=1500; total time=   1.0s\n",
      "[CV] END gamma=0.4, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=1500; total time=   1.0s\n",
      "[CV] END gamma=0.4, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=1500; total time=   1.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=6, min_child_weight=1, n_estimators=1500; total time=   1.0s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=6, min_child_weight=1, n_estimators=1500; total time=   1.1s\n",
      "[CV] END gamma=0.2, learning_rate=0.15, max_depth=6, min_child_weight=1, n_estimators=1500; total time=   1.0s\n",
      "[CV] END gamma=0.4, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=1100; total time=   0.7s\n",
      "[CV] END gamma=0.4, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=1100; total time=   0.7s\n",
      "[CV] END gamma=0.4, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=1100; total time=   0.8s\n",
      "[CV] END gamma=0.0, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500; total time=   0.3s\n",
      "[CV] END gamma=0.0, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500; total time=   0.3s\n",
      "[CV] END gamma=0.0, learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=500; total time=   0.3s\n",
      "{'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.05, 'gamma': 0.2}\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      1.00      0.70        57\n",
      "         1.0       1.00      0.09      0.17        54\n",
      "\n",
      "    accuracy                           0.56       111\n",
      "   macro avg       0.77      0.55      0.43       111\n",
      "weighted avg       0.76      0.56      0.44       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=Y_train \n",
    ")\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = { \"learning_rate\" : [0.05,0.1,0.15,0.20],\n",
    "                 \"n_estimators\" : [100, 500, 900, 1100, 1500],\n",
    "                 \"max_depth\" : [ 3, 5, 6, 10, 15],\n",
    "                 \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "                 \"gamma\" : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ]}\n",
    "\n",
    "grid = RandomizedSearchCV(xgb, random_grid, cv=3, random_state=42, refit = True, verbose = 2)\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, Y_train, sample_weight=sample_weights)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "grid_predictions_xgb = grid.predict(X_test)\n",
    "  \n",
    "# print classification report\n",
    "print(classification_report(Y_test, grid_predictions_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb531ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
